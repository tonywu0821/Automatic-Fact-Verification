{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_symbol(text):\n",
    "    temp_text = text.replace(\"_\",\" \")\n",
    "    temp_text = temp_text.replace(\"-LRB-\",\"\")\n",
    "    temp_text = temp_text.replace(\"-RRB-\",\"\")\n",
    "    temp_text = temp_text.replace(\"-LSB-\",\"\")\n",
    "    temp_text = temp_text.replace(\"-RSB-\",\"\")\n",
    "    temp_text = temp_text.replace(\"\\\\\",\"\")\n",
    "    temp_text = temp_text.replace(\"/\",\"\")\n",
    "    temp_text = temp_text.replace(\")\",\"\")   \n",
    "    temp_text = temp_text.replace(\"(\",\"\") \n",
    "    return temp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "wiki = []\n",
    "topics = {}\n",
    "replaced_topics = []\n",
    "topic_list = []\n",
    "\n",
    "error_sentence = 0\n",
    "\n",
    "time_start_out = time.time()\n",
    "document_num = 109\n",
    "for k in range(document_num):\n",
    "    #number = str(i+1)\n",
    "    #filename = 'wiki-pages-text/wiki-%s.txt' % (number.zfill(3))\n",
    "    filename = 'wiki-pages-text/wiki-%03d.txt' % (k+1)    \n",
    "    with open(filename, 'r', encoding='UTF-8') as f:\n",
    "        content_lines = f.readlines() \n",
    "        time_start = time.time()\n",
    "        print('processing %d' % (k+1))\n",
    "        for i in range(len(content_lines)):\n",
    "        #for i in range(10):\n",
    "            wiki_sentence = content_lines[i].split(\" \",2)\n",
    "            topic = wiki_sentence[0]\n",
    "            \n",
    "            if (not wiki_sentence[1].isdigit()):\n",
    "                error_sentence +=1\n",
    "                continue\n",
    "                \n",
    "            sentence_number = int(wiki_sentence[1])\n",
    "            if topic not in topics:\n",
    "                topic_id = len(topics)\n",
    "                topics[topic] = topic_id\n",
    "                topic_list.append(topic)\n",
    "                \n",
    "                replaced_topic = replace_symbol(topic)\n",
    "                replaced_topics.append(replaced_topic)\n",
    "                content = replace_symbol(wiki_sentence[2])\n",
    "                wiki.append({sentence_number:content})\n",
    "            else :\n",
    "                topic_id = topics[topic]\n",
    "                content = replace_symbol(wiki_sentence[2])\n",
    "                wiki[topic_id][sentence_number] = content\n",
    "        time_end = time.time()\n",
    "        cost_time = time_end - time_start\n",
    "        print('time cost',cost_time,'s')\n",
    "time_end_out = time.time()\n",
    "avg_cosumed_time = (time_end_out-time_start_out)/document_num\n",
    "print('average time', avg_cosumed_time, 's')\n",
    "print('error_sentence_count',error_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## block for generate inverted index\n",
    "\n",
    "import os\n",
    "import lucene\n",
    "from java.io import File\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.document import Document, Field, FieldType\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexOptions, IndexReader, DirectoryReader\n",
    "from org.apache.lucene.store import SimpleFSDirectory, FSDirectory\n",
    "from org.apache.lucene.util import Version\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "\n",
    "lucene.initVM()\n",
    "INDEX_DIR = 'index2/'\n",
    "store = SimpleFSDirectory(File(INDEX_DIR).toPath())\n",
    "index_writer = IndexWriter(store, IndexWriterConfig(StandardAnalyzer()))\n",
    "\n",
    "t1 = FieldType()\n",
    "t1.setOmitNorms(False)\n",
    "t1.setStored(True)\n",
    "t1.setTokenized(True)\n",
    "t1.setIndexOptions(IndexOptions.DOCS_AND_FREQS)\n",
    "\n",
    "time_start_generate = time.time()\n",
    "\n",
    "for n, content in enumerate(replaced_topics):  # read in doc here\n",
    "    doc = Document()\n",
    "    doc.add(Field(\"topic_id\", str(n), t1))\n",
    "    doc.add(Field(\"topic_content\", content, t1))\n",
    "    index_writer.addDocument(doc)   # Add the document to the index\n",
    "\n",
    "print(\"%r docs in index\" % index_writer.numDocs())\n",
    "index_writer.commit()\n",
    "index_writer.close()\n",
    "time_end_generate = time.time()\n",
    "print(\"take \", time_end_generate - time_start_generate, \" s to generate inverted index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "testing_set = json.load(open('test-unlabelled.json'))\n",
    "testing_set_out = json.load(open('test-unlabelled.json'))\n",
    "#testing_set = json.load(open('devset.json'))\n",
    "#testing_set_out = json.load(open('devset.json'))\n",
    "#testing_set = json.load(open('test_set_500.json'))\n",
    "#testing_set_out = json.load(open('test_set_500.json'))\n",
    "\n",
    "print(len(testing_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## indentify name entity or Pos and generate the query\n",
    "## block for searching index and comparing \n",
    "searcher = IndexSearcher(DirectoryReader.open(store))\n",
    "\n",
    "process_counter = 0\n",
    "time_start_process = time.time()\n",
    "for key, value in testing_set.items():\n",
    "    process_counter += 1\n",
    "    claim = value['claim']\n",
    "    claim = replace_symbol(claim)\n",
    "    claim_doc = nlp(claim)\n",
    "    \n",
    "    if process_counter % 100 == 0:\n",
    "        print(\"have processed:\",process_counter)\n",
    "        time_end_process = time.time()\n",
    "        print(process_counter/(time_end_process-time_start_process))\n",
    "    \n",
    "    query = \"\" \n",
    "    \n",
    "    for token in claim_doc:\n",
    "        if token.text[0].isupper() and token.pos_ != \"DET\":\n",
    "            query += (token.text + \" \")\n",
    "        elif token.pos_ in {\"NOUN\",'PROPN','NUM'}:\n",
    "            if token.pos_ != 'NUM' or token.text.isalnum() :\n",
    "                query += (token.text + \" \")\n",
    "    \n",
    "    if(len(query) == 0):\n",
    "        print(\"this claim has no query:\", claim)\n",
    "        query = claim\n",
    "    \n",
    "    my_query = QueryParser(\"topic_content\", StandardAnalyzer()).parse(query)                                                            # type query here\n",
    "    MAX = 1\n",
    "    total_hits = searcher.search(my_query, MAX).scoreDocs\n",
    "    \n",
    "    candidate_evidences = []\n",
    "    for hit in total_hits:\n",
    "        hit_doc = searcher.doc(hit.doc)\n",
    "        hit_doc_id = int(hit_doc['topic_id'])\n",
    "        hit_doc_topic = topic_list[hit_doc_id]\n",
    "        \n",
    "        for sentence_num, content in wiki[hit_doc_id].items():\n",
    "\n",
    "            candidate_evidence = nlp(content)\n",
    "            \n",
    "            # some evidence is empty ?\n",
    "            if candidate_evidence.vector_norm == 0:\n",
    "                #print(candidate_evidence)\n",
    "                continue\n",
    "            \n",
    "            sim = claim_doc.similarity(candidate_evidence)\n",
    "            \n",
    "            if sim > 0.79:\n",
    "                candidate_evidences.append([hit_doc_topic,int(sentence_num)])\n",
    "                \n",
    "    if(len(candidate_evidences) > 0 ):\n",
    "        testing_set_out[key]['label'] = 'SUPPORTS'\n",
    "        testing_set_out[key]['evidence'] = candidate_evidences\n",
    "    else:\n",
    "        testing_set_out[key]['label'] = 'NOT ENOUGH INFO'\n",
    "        testing_set_out[key]['evidence'] = candidate_evidences\n",
    "\n",
    "time_end_process = time.time()\n",
    "time_cost_process = time_end_process - time_start_process\n",
    "print(\"take\",time_cost_process,\" s to process\")\n",
    "print(\"avg:\",time_cost_process/process_counter,\"s\")\n",
    "with open('testoutput.json', 'w') as f:\n",
    "    json.dump(testing_set_out, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
