{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_symbol(text):\n",
    "    temp_text = text.replace(\"_\",\" \")\n",
    "    temp_text = temp_text.replace(\"-LRB-\",\"\")\n",
    "    temp_text = temp_text.replace(\"-RRB-\",\"\")\n",
    "    temp_text = temp_text.replace(\"-LSB-\",\"\")\n",
    "    temp_text = temp_text.replace(\"-RSB-\",\"\")\n",
    "    temp_text = temp_text.replace(\"\\\\\",\"\")\n",
    "    temp_text = temp_text.replace(\"/\",\"\")\n",
    "    temp_text = temp_text.replace(\")\",\"\")   \n",
    "    temp_text = temp_text.replace(\"(\",\"\") \n",
    "    return temp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "wiki = []\n",
    "topics = {}\n",
    "replaced_topics = []\n",
    "topic_list = []\n",
    "\n",
    "error_sentence = 0\n",
    "\n",
    "time_start_out = time.time()\n",
    "document_num = 109\n",
    "for k in range(document_num):\n",
    "    #number = str(i+1)\n",
    "    #filename = 'wiki-pages-text/wiki-%s.txt' % (number.zfill(3))\n",
    "    filename = 'wiki-pages-text/wiki-%03d.txt' % (k+1)    \n",
    "    with open(filename, 'r', encoding='UTF-8') as f:\n",
    "        content_lines = f.readlines() \n",
    "        time_start = time.time()\n",
    "        print('processing %d' % (k+1))\n",
    "        for i in range(len(content_lines)):\n",
    "        #for i in range(10):\n",
    "            wiki_sentence = content_lines[i].split(\" \",2)\n",
    "            topic = wiki_sentence[0]\n",
    "            \n",
    "            if (not wiki_sentence[1].isdigit()):\n",
    "                error_sentence +=1\n",
    "                continue\n",
    "                \n",
    "            sentence_number = int(wiki_sentence[1])\n",
    "            if topic not in topics:\n",
    "                topic_id = len(topics)\n",
    "                topics[topic] = topic_id\n",
    "                \n",
    "                topic_list.append(topic)\n",
    "                                \n",
    "                replaced_topic = replace_symbol(topic)\n",
    "                replaced_topics.append(replaced_topic)\n",
    "                \n",
    "                content = replace_symbol(wiki_sentence[2])\n",
    "                wiki.append({sentence_number:content})\n",
    "            else :\n",
    "                topic_id = topics[topic]\n",
    "                \n",
    "                content = replace_symbol(wiki_sentence[2])\n",
    "                \n",
    "                wiki[topic_id][sentence_number] = content\n",
    "                \n",
    "        time_end = time.time()\n",
    "        cost_time = time_end - time_start\n",
    "        print('time cost',cost_time,'s')\n",
    "time_end_out = time.time()\n",
    "avg_cosumed_time = (time_end_out-time_start_out)/document_num\n",
    "print('average time', avg_cosumed_time, 's')\n",
    "print('error_sentence_count',error_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## block for generate inverted index\n",
    "\n",
    "## if inverted index is already generated, you need to delete the index2/ folder.\n",
    "\n",
    "import os\n",
    "import lucene\n",
    "from java.io import File\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.document import Document, Field, FieldType\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexOptions, IndexReader, DirectoryReader\n",
    "from org.apache.lucene.store import SimpleFSDirectory, FSDirectory\n",
    "from org.apache.lucene.util import Version\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "\n",
    "lucene.initVM()\n",
    "INDEX_DIR = 'index2/'\n",
    "store = SimpleFSDirectory(File(INDEX_DIR).toPath())\n",
    "index_writer = IndexWriter(store, IndexWriterConfig(StandardAnalyzer()))\n",
    "\n",
    "t1 = FieldType()\n",
    "t1.setOmitNorms(False)\n",
    "t1.setStored(True)\n",
    "t1.setTokenized(True)\n",
    "t1.setIndexOptions(IndexOptions.DOCS_AND_FREQS)\n",
    "\n",
    "time_start_generate = time.time()\n",
    "\n",
    "for n, content in enumerate(replaced_topics):  # read in doc here\n",
    "    doc = Document()\n",
    "    doc.add(Field(\"topic_id\", str(n), t1))\n",
    "    doc.add(Field(\"topic_content\", content, t1))    \n",
    "\n",
    "    index_writer.addDocument(doc)   # Add the document to the index\n",
    "\n",
    "print(\"%r docs in index\" % index_writer.numDocs())\n",
    "index_writer.commit()\n",
    "index_writer.close()\n",
    "time_end_generate = time.time()\n",
    "print(\"take \", time_end_generate - time_start_generate, \" s to generate inverted index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## block for import test dataset\n",
    "import json\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "testing_set = json.load(open('test-unlabelled.json'))\n",
    "testing_set_out = json.load(open('test-unlabelled.json'))\n",
    "#testing_set = json.load(open('devset.json'))\n",
    "#testing_set_out = json.load(open('devset.json'))\n",
    "print(len(testing_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## indentify name entity generate the query\n",
    "## block for searching index and comparing\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "## The pretrain model\n",
    "#predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz\")\n",
    "\n",
    "## The self-trained model\n",
    "predictor = Predictor.from_path(\"model.tar.gz\") # 1 2 0\n",
    "\n",
    "#if you hava cuda, uncomment the below line\n",
    "#predictor._model = predictor._model.cuda() \n",
    "\n",
    "searcher = IndexSearcher(DirectoryReader.open(store))\n",
    "\n",
    "process_counter = 0\n",
    "time_start_process = time.time()\n",
    "for key, value in testing_set.items():\n",
    "    \n",
    "    process_counter += 1\n",
    "    claim = value['claim']\n",
    "    claim = replace_symbol(claim)\n",
    "    claim_doc = nlp(claim)\n",
    "    \n",
    "    if process_counter % 100 == 0:\n",
    "        print(\"have processed:\",process_counter)\n",
    "        time_end_process = time.time()\n",
    "        print(\"speed(instances/s):\",process_counter/(time_end_process-time_start_process))\n",
    "    \n",
    "    query = \"\" \n",
    "    \n",
    "    for token in claim_doc:\n",
    "        if token.text[0].isupper() and token.pos_ != \"DET\":\n",
    "            query += (token.text + \" \")\n",
    "        elif token.pos_ in {\"NOUN\",'PROPN','NUM'}:\n",
    "            if token.pos_ != 'NUM' or token.text.isalnum():\n",
    "                query += (token.text + \" \")\n",
    "\n",
    "    if(len(query) == 0):\n",
    "        print(\"this claim has no query:\", claim)\n",
    "        query = claim\n",
    "    \n",
    "    my_query = QueryParser(\"topic_content\", StandardAnalyzer()).parse(query)                                                            # type query here\n",
    "    MAX = 1\n",
    "    total_hits = searcher.search(my_query, MAX).scoreDocs\n",
    "\n",
    "    sup_evidence = []\n",
    "    refu_evidence = []\n",
    "    not_enough = []\n",
    "    \n",
    "    time_start = time.time()\n",
    "    for hit in total_hits:\n",
    "\n",
    "        hit_doc = searcher.doc(hit.doc)\n",
    "        hit_doc_id = int(hit_doc['topic_id'])\n",
    "        hit_doc_topic = topic_list[hit_doc_id]\n",
    "        \n",
    "        for sentence_num, content in wiki[hit_doc_id].items():\n",
    "                           \n",
    "            result = predictor.predict(\n",
    "                  hypothesis = claim ,\n",
    "                  premise = content\n",
    "                )\n",
    "            label_probs = result['label_probs']\n",
    "            \n",
    "            max_index = label_probs.index(max(label_probs))\n",
    "            if max_index == 1:\n",
    "                sup_evidence.append([hit_doc_topic,sentence_num]) \n",
    "            elif max_index == 2:\n",
    "                refu_evidence.append([hit_doc_topic,sentence_num])\n",
    "            elif max_index == 0:\n",
    "                not_enough.append([hit_doc_topic,sentence_num])\n",
    "\n",
    "    vote = [len(sup_evidence),len(refu_evidence)]\n",
    "    if(len(sup_evidence) == 0 and len(refu_evidence) == 0):\n",
    "        testing_set_out[key]['label'] = 'NOT ENOUGH INFO'\n",
    "        testing_set_out[key]['evidence'] = []\n",
    "        #print('NOT ENOUGH INFO')\n",
    "        continue\n",
    "    \n",
    "    index_vote = vote.index(max(vote))\n",
    "    #print(index_vote)\n",
    "    if(index_vote == 0):\n",
    "        #print('SUPPORTS')\n",
    "        testing_set_out[key]['label'] = 'SUPPORTS'\n",
    "        testing_set_out[key]['evidence'] = sup_evidence[:5]\n",
    "    elif(index_vote == 1):\n",
    "        #print('REFUTES')\n",
    "        testing_set_out[key]['label'] = 'REFUTES'\n",
    "        testing_set_out[key]['evidence'] = refu_evidence[:5]\n",
    "\n",
    "time_end_process = time.time()\n",
    "time_cost_process = time_end_process - time_start_process\n",
    "print(\"take\",time_cost_process,\" s to process\")\n",
    "print(\"avg:\",time_cost_process/process_counter,\"s\")\n",
    "with open('testoutput.json', 'w') as f:\n",
    "    json.dump(testing_set_out, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
